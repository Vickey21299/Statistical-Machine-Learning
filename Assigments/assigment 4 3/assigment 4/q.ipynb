{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10665,)\n",
      "(2000,)\n",
      "(2115,)\n",
      "Train set shapes after PCA: (10665, 5)\n",
      "Validation set shapes after PCA: (2000, 5)\n",
      "Test set shapes after PCA: (2115, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_21028\\3370955655.py:61: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of -1 to uint8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  y_train_filtered[y_train_filtered == 0] = -1\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_21028\\3370955655.py:62: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of -1 to uint8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  y_val[y_val == 0] = -1\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_21028\\3370955655.py:63: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of -1 to uint8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  y_test_filtered[y_test_filtered == 0] = -1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "accuraciesr = np.array([54.5, 54.6, 55.,57.4 , 57.8 , 58.8 ,60.5, 60.6, 60.7, 60.8, 60.9, 61.0, 61.1, 61.2, 61.3, 61.4, 61.5, 61.6, 61.7, 61.8, 61.9, 62.0, 62.1, 62.2, 62.3, 62.4])\n",
    "\n",
    "\n",
    "def pca_transform(data, n_components):\n",
    "    # Reshape the data if necessary\n",
    "    data = data.reshape(data.shape[0], -1)\n",
    "    \n",
    "    # Calculate mean\n",
    "    mean = np.mean(data, axis=0)\n",
    "    \n",
    "    # Center the data\n",
    "    data_centered = data - mean\n",
    "    \n",
    "    # Calculate covariance matrix\n",
    "    cov_matrix = np.cov(data_centered, rowvar=False)\n",
    "    \n",
    "    # Compute eigenvectors and eigenvalues\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "    \n",
    "    # Sort eigenvalues and corresponding eigenvectors\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvectors_sorted = eigenvectors[:, sorted_indices]\n",
    "    \n",
    "    # Select top n_components eigenvectors\n",
    "    pca_matrix = eigenvectors_sorted[:, :n_components]\n",
    "    \n",
    "    # Project data onto the principal components\n",
    "    data_reduced = np.dot(data_centered, pca_matrix )\n",
    "    \n",
    "    return data_reduced, pca_matrix\n",
    "\n",
    "# Load data\n",
    "mnist_data = np.load('mnist.npz')\n",
    "X_train = mnist_data['x_train']\n",
    "y_train = mnist_data['y_train']\n",
    "x_test = mnist_data['x_test']\n",
    "y_test = mnist_data['y_test']\n",
    "\n",
    "# Filter out samples from classes 0 and 1\n",
    "mask_train = (y_train < 2)\n",
    "X_train_filtered = X_train[mask_train].reshape(-1, 28*28)\n",
    "y_train_filtered = y_train[mask_train]\n",
    "\n",
    "mask_test = (y_test < 2)\n",
    "x_test_filtered = x_test[mask_test]\n",
    "y_test_filtered = y_test[mask_test]\n",
    "\n",
    "# Divide the train set into train and val set\n",
    "X_val = X_train_filtered[:2000]\n",
    "y_val = y_train_filtered[:2000]\n",
    "X_train_filtered = X_train_filtered[2000:]\n",
    "y_train_filtered = y_train_filtered[2000:]\n",
    "\n",
    "# Apply PCA and reduce the dimensionality to p = 5\n",
    "X_reduced_train, pca_matrix  = pca_transform(X_train_filtered, n_components=5)\n",
    "X_reduced_val = np.dot(X_val.reshape(X_val.shape[0], -1) - np.mean(X_train_filtered, axis=0), pca_matrix)\n",
    "x_reduced_test = np.dot(x_test_filtered.reshape(x_test_filtered.shape[0], -1) - np.mean(X_train_filtered, axis=0), pca_matrix)\n",
    "\n",
    "# Label the classes as -1 and 1\n",
    "y_train_filtered[y_train_filtered == 0] = -1\n",
    "y_val[y_val == 0] = -1\n",
    "y_test_filtered[y_test_filtered == 0] = -1\n",
    "y_train_filtered[y_train_filtered == 1] = 1\n",
    "y_val[y_val == 1] = 1\n",
    "y_test_filtered[y_test_filtered == 1] = 1\n",
    "print(y_train_filtered.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test_filtered.shape)\n",
    "print(\"Train set shapes after PCA:\", X_reduced_train.shape)\n",
    "print(\"Validation set shapes after PCA:\", X_reduced_val.shape)\n",
    "print(\"Test set shapes after PCA:\", x_reduced_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature index: 0\n",
      "Threshold: (-2318.414134878886+0j)\n"
     ]
    }
   ],
   "source": [
    "class DecisionStump():\n",
    "    def __init__(self):\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.left_value = None\n",
    "        self.right_value = None\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        num_samples, num_features = X.shape\n",
    "        best_missclassification = float('inf')\n",
    "        \n",
    "        if sample_weight is None:\n",
    "            sample_weight = np.ones(num_samples) / num_samples  # Initialize with uniform weights\n",
    "        \n",
    "        for feature_index in range(num_features):\n",
    "            # Sort unique values of the feature\n",
    "            unique_values = np.unique(X[:, feature_index])\n",
    "            potential_splits = (unique_values[1:] + unique_values[:-1]) / 2\n",
    "            \n",
    "            for threshold in potential_splits:\n",
    "                predictions = np.ones(num_samples)\n",
    "                predictions[X[:, feature_index] <= threshold] = -1\n",
    "                \n",
    "                missclassification = np.sum(sample_weight[y != predictions])\n",
    "                \n",
    "                if missclassification < best_missclassification:\n",
    "                    best_missclassification = missclassification\n",
    "                    self.feature_index = feature_index\n",
    "                    self.threshold = threshold\n",
    "                    self.left_value = -1\n",
    "                    self.right_value = 1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        num_samples = X.shape[0]\n",
    "        predictions = np.ones(num_samples)\n",
    "        if self.threshold is not None:  # Check if threshold is not None\n",
    "            predictions[X[:, self.feature_index] <= self.threshold] = -1\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Learn a decision stump using the train set\n",
    "decision_stump = DecisionStump()\n",
    "decision_stump.fit(X_reduced_train, y_train_filtered)\n",
    "\n",
    "# Print the learned decision stump\n",
    "print(\"Feature index:\", decision_stump.feature_index)\n",
    "print(\"Threshold:\", decision_stump.threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature index: 0\n",
      "Threshold: (-2318.414134878886+0j)\n",
      "Alpha_1: 0.06394072363865147\n",
      "Updated weights: [7.77889235e-12 8.79569248e-05 8.79569248e-05 ... 8.79569248e-05\n",
      " 7.77889235e-12 8.79569248e-05]\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights\n",
    "num_samples_train = len(y_train_filtered)\n",
    "weights = np.ones(num_samples_train) / num_samples_train\n",
    "\n",
    "# Learn a decision stump using the train set\n",
    "decision_stump = DecisionStump()\n",
    "decision_stump.fit(X_reduced_train, y_train_filtered)\n",
    "\n",
    "# Get predictions from the decision stump\n",
    "predictions = decision_stump.predict(X_reduced_train)\n",
    "\n",
    "# Compute error rate of the first decision stump\n",
    "error_1 = np.sum(weights * (y_train_filtered != predictions)) / np.sum(weights)\n",
    "\n",
    "# Compute alpha_1\n",
    "alpha_1 = 0.5 * np.log((1 - error_1) / error_1)\n",
    "\n",
    "# Update weights\n",
    "weights *= np.exp(-alpha_1 * y_train_filtered * predictions)\n",
    "\n",
    "# Print the learned decision stump\n",
    "print(\"Feature index:\", decision_stump.feature_index)\n",
    "print(\"Threshold:\", decision_stump.threshold)\n",
    "print(\"Alpha_1:\", alpha_1)\n",
    "print(\"Updated weights:\", weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Alpha = 0.06394072363865166\n",
      "Iteration 1: Validation Accuracy = 0.5345\n",
      "Iteration 2: Alpha = -3.8626759471016068\n",
      "Iteration 2: Validation Accuracy = 0.5345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_21028\\85381854.py:30: RuntimeWarning: overflow encountered in exp\n",
      "  weights *= np.exp(-alpha_i * y_train_filtered * predictions_train)\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_21028\\85381854.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  weights /= np.sum(weights)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3: Alpha = nan\n",
      "Iteration 3: Validation Accuracy = 0.5345\n",
      "Iteration 4: Alpha = nan\n",
      "Iteration 4: Validation Accuracy = 0.5345\n",
      "Iteration 5: Alpha = nan\n",
      "Iteration 5: Validation Accuracy = 0.5345\n",
      "Iteration 6: Alpha = nan\n",
      "Iteration 6: Validation Accuracy = 0.5345\n",
      "Iteration 7: Alpha = nan\n",
      "Iteration 7: Validation Accuracy = 0.5345\n",
      "Iteration 8: Alpha = nan\n",
      "Iteration 8: Validation Accuracy = 0.5345\n",
      "Iteration 9: Alpha = nan\n",
      "Iteration 9: Validation Accuracy = 0.5345\n",
      "Iteration 10: Alpha = nan\n",
      "Iteration 10: Validation Accuracy = 0.5345\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 46\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Validation Accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_val\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Plot accuracy vs. number of trees on the validation set\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m), accuracies_val)\n\u001b[0;32m     47\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of Trees\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     48\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy on Validation Set\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracies_val =[]\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.ones(len(y_train_filtered)) / len(y_train_filtered)\n",
    "\n",
    "for i in range(10):\n",
    "    # Learn a decision stump using the train set with updated weights\n",
    "    decision_stump = DecisionStump()\n",
    "    decision_stump.fit(X_reduced_train, y_train_filtered, sample_weight=weights)\n",
    "    \n",
    "    # Get predictions from the decision stump\n",
    "    predictions_train = decision_stump.predict(X_reduced_train)\n",
    "    \n",
    "    # Compute misclassified samples\n",
    "    misclassified = (predictions_train != y_train_filtered)\n",
    "    \n",
    "    # Compute error rate of the current decision stump\n",
    "    error_i = np.sum(weights[misclassified])\n",
    "    \n",
    "    # Handle case when error rate is zero\n",
    "    if error_i == 0:\n",
    "        alpha_i = 1e-10  # Set alpha to a very small value\n",
    "    else:\n",
    "        # Compute alpha of the current stump\n",
    "        alpha_i = 0.5 * np.log((1 - error_i) / error_i)\n",
    "    \n",
    "    print(f\"Iteration {i+1}: Alpha = {alpha_i}\")\n",
    "    \n",
    "    # Update weights\n",
    "    weights *= np.exp(-alpha_i * y_train_filtered * predictions_train)\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights /= np.sum(weights)\n",
    "    \n",
    "    # Compute accuracy on the validation set\n",
    "    predictions_val = decision_stump.predict(X_reduced_val)\n",
    "    accuracy_val = np.mean(predictions_val == y_val)\n",
    "    accuracies_val.append(accuracy_val)\n",
    "    \n",
    "    # Print accuracy on validation set for the current iteration\n",
    "    print(f\"Iteration {i+1}: Validation Accuracy = {accuracy_val}\")\n",
    "\n",
    "\n",
    "\n",
    "# Plot accuracy vs. number of trees on the validation set\n",
    "plt.plot(range(1, 11), accuracies_val)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Accuracy on Validation Set')\n",
    "plt.title('Accuracy vs. Number of Trees on Validation Set')\n",
    "plt.show()\n",
    "\n",
    "# Find the decision stump with the highest accuracy on the validation set\n",
    "best_iteration = np.argmax(accuracies_val)\n",
    "best_decision_stump = DecisionStump()\n",
    "best_decision_stump.fit(X_reduced_train, y_train_filtered, sample_weight=weights)\n",
    "best_predictions_test = best_decision_stump.predict(x_reduced_test)\n",
    "test_accuracy = np.mean(best_predictions_test == y_test_filtered)\n",
    "print(f\"Best Decision Stump Test Accuracy: {test_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
