{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "array = np.load('x_train.npy')\n",
    "\n",
    "print(array[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10665,)\n",
      "(2000,)\n",
      "(2115,)\n",
      "Train set shapes after PCA: (10665, 5)\n",
      "Validation set shapes after PCA: (2000, 5)\n",
      "Test set shapes after PCA: (2115, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_13752\\1422769840.py:59: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of -1 to uint8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  y_train_filtered[y_train_filtered == 0] = -1\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_13752\\1422769840.py:60: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of -1 to uint8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  y_val[y_val == 0] = -1\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_13752\\1422769840.py:61: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of -1 to uint8 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)\n",
      "will give the desired result (the cast overflows).\n",
      "  y_test_filtered[y_test_filtered == 0] = -1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pca_transform(data, n_components):\n",
    "    # Reshape the data if necessary\n",
    "    data = data.reshape(data.shape[0], -1)\n",
    "    \n",
    "    # Calculate mean\n",
    "    mean = np.mean(data, axis=0)\n",
    "    \n",
    "    # Center the data\n",
    "    data_centered = data - mean\n",
    "    \n",
    "    # Calculate covariance matrix\n",
    "    cov_matrix = np.cov(data_centered, rowvar=False)\n",
    "    \n",
    "    # Compute eigenvectors and eigenvalues\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "    \n",
    "    # Sort eigenvalues and corresponding eigenvectors\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvectors_sorted = eigenvectors[:, sorted_indices]\n",
    "    \n",
    "    # Select top n_components eigenvectors\n",
    "    pca_matrix = eigenvectors_sorted[:, :n_components]\n",
    "    \n",
    "    # Project data onto the principal components\n",
    "    data_reduced = np.dot(data_centered, pca_matrix)\n",
    "    \n",
    "    return data_reduced, pca_matrix\n",
    "\n",
    "# Load data\n",
    "mnist_data = np.load('mnist.npz')\n",
    "X_train = mnist_data['x_train']\n",
    "y_train = mnist_data['y_train']\n",
    "x_test = mnist_data['x_test']\n",
    "y_test = mnist_data['y_test']\n",
    "\n",
    "# Filter out samples from classes 0 and 1\n",
    "mask_train = (y_train < 2)\n",
    "X_train_filtered = X_train[mask_train].reshape(-1, 28*28)\n",
    "y_train_filtered = y_train[mask_train]\n",
    "\n",
    "mask_test = (y_test < 2)\n",
    "x_test_filtered = x_test[mask_test]\n",
    "y_test_filtered = y_test[mask_test]\n",
    "\n",
    "# Divide the train set into train and val set\n",
    "X_val = X_train_filtered[:2000]\n",
    "y_val = y_train_filtered[:2000]\n",
    "X_train_filtered = X_train_filtered[2000:]\n",
    "y_train_filtered = y_train_filtered[2000:]\n",
    "\n",
    "# Apply PCA and reduce the dimensionality to p = 5\n",
    "X_reduced_train, pca_matrix = pca_transform(X_train_filtered, n_components=5)\n",
    "X_reduced_val = np.dot(X_val.reshape(X_val.shape[0], -1) - np.mean(X_train_filtered, axis=0), pca_matrix)\n",
    "x_reduced_test = np.dot(x_test_filtered.reshape(x_test_filtered.shape[0], -1) - np.mean(X_train_filtered, axis=0), pca_matrix)\n",
    "\n",
    "# Label the classes as -1 and 1\n",
    "y_train_filtered[y_train_filtered == 0] = -1\n",
    "y_val[y_val == 0] = -1\n",
    "y_test_filtered[y_test_filtered == 0] = -1\n",
    "y_train_filtered[y_train_filtered == 1] = 1\n",
    "y_val[y_val == 1] = 1\n",
    "y_test_filtered[y_test_filtered == 1] = 1\n",
    "print(y_train_filtered.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test_filtered.shape)\n",
    "print(\"Train set shapes after PCA:\", X_reduced_train.shape)\n",
    "print(\"Validation set shapes after PCA:\", X_reduced_val.shape)\n",
    "print(\"Test set shapes after PCA:\", x_reduced_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTreeClassifier():\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        self.root = None\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth \n",
    "    \n",
    "    def build_tree(self, dataset, curr_depth=0):\n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "\n",
    "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            if best_split[\"info_gain\"]>0:\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            for threshold in possible_thresholds:\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n",
    "                    if curr_info_gain>max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "    \n",
    "        return best_split\n",
    "    \n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):\n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        if mode==\"gini\":\n",
    "            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n",
    "        else:\n",
    "            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n",
    "        return gain\n",
    "    \n",
    "    def gini_index(self, y): \n",
    "        class_labels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            gini += p_cls**2\n",
    "        return 1 - gini\n",
    "        \n",
    "    def calculate_leaf_value(self, Y):   \n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "    \n",
    "    def fit(self, X, Y, sample_weight=None):  \n",
    "        if sample_weight is None:\n",
    "            dataset = np.concatenate((X, Y.reshape(-1, 1)), axis=1)\n",
    "        else:\n",
    "            dataset = np.concatenate((X, Y.reshape(-1, 1), sample_weight.reshape(-1, 1)), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "    \n",
    "    def predict(self, X):  \n",
    "        predictions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return predictions\n",
    "    \n",
    "    def make_prediction(self, x, tree):\n",
    "        if tree.value != None:\n",
    "            return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val <= tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_reduced and y_train are your datasets\n",
    "weights = np.ones(len(y_train[:1000]))  # Initialize weights uniformly\n",
    "decision_stump = DecisionTreeClassifier(min_samples_split=2, max_depth=1)\n",
    "decision_stump.fit(X_reduced[:1000], y_train[:1000])\n",
    "predictions = decision_stump.predict(X_reduced[:1000])\n",
    "\n",
    "# Calculate error rate\n",
    "error_rate = np.sum(weights * (predictions != y_train[:1000])) / np.sum(weights)\n",
    "\n",
    "# Calculate alpha (α1)\n",
    "alpha = 0.5 * np.log((1 - error_rate) / error_rate)\n",
    "\n",
    "# Update weights\n",
    "for i in range(len(weights)):\n",
    "    if predictions[i] == y_train[i]:\n",
    "        weights[i] *= np.exp(-alpha)\n",
    "    else:\n",
    "        weights[i] *= np.exp(alpha)\n",
    "\n",
    "# Normalize weights\n",
    "weights /= np.sum(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha (α1): 3.1063030478757594\n",
      "Updated weights: [0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.25     0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.25     0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501\n",
      " 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501 0.000501]\n"
     ]
    }
   ],
   "source": [
    "# Print the computed alpha (α1)\n",
    "print(\"Alpha (α1):\", alpha)\n",
    "\n",
    "# Print the updated weights\n",
    "print(\"Updated weights:\", weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (3000, 5)\n",
      "X shape (2000, 5)\n",
      "Iteration 1: Alpha = 0.06676569631226129, h(1) = <__main__.DecisionTreeClassifier object at 0x000001D7DA521B80>, Validation Accuracy = 0.5345\n",
      "X shape (3000, 5)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 105\u001b[0m\n\u001b[0;32m    103\u001b[0m decision_stump \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    104\u001b[0m decision_stump\u001b[38;5;241m.\u001b[39mfit(X_reduced[:\u001b[38;5;241m3000\u001b[39m], y_train[:\u001b[38;5;241m3000\u001b[39m], sample_weight\u001b[38;5;241m=\u001b[39mweights)  \u001b[38;5;66;03m# Use updated weights\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mdecision_stump\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_reduced\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Calculate error rate\u001b[39;00m\n\u001b[0;32m    108\u001b[0m error_rate \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(weights \u001b[38;5;241m*\u001b[39m (predictions \u001b[38;5;241m!=\u001b[39m y_train[:\u001b[38;5;241m3000\u001b[39m])) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(weights)\n",
      "Cell \u001b[1;32mIn[65], line 79\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):  \n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX shape\u001b[39m\u001b[38;5;124m\"\u001b[39m,X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 79\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X]\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "Cell \u001b[1;32mIn[65], line 85\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.make_prediction\u001b[1;34m(self, x, tree)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m---> 85\u001b[0m feature_val \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_val \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mthreshold:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_prediction(x, tree\u001b[38;5;241m.\u001b[39mleft)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 5 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTreeClassifier():\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        self.root = None\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth \n",
    "    \n",
    "    def build_tree(self, dataset, curr_depth=0):\n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "\n",
    "        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            if best_split[\"info_gain\"]>0:\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            for threshold in possible_thresholds:\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y, \"gini\")\n",
    "                    if curr_info_gain>max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "    \n",
    "        return best_split\n",
    "    \n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):\n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        if mode==\"gini\":\n",
    "            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n",
    "        else:\n",
    "            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n",
    "        return gain\n",
    "    \n",
    "    def gini_index(self, y): \n",
    "        class_labels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            gini += p_cls**2\n",
    "        return 1 - gini\n",
    "        \n",
    "    def calculate_leaf_value(self, Y):   \n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "    \n",
    "    def fit(self, X, Y, sample_weight=None):  \n",
    "        if sample_weight is None:\n",
    "            dataset = np.concatenate((X, Y.reshape(-1, 1)), axis=1)\n",
    "        else:\n",
    "            dataset = np.concatenate((X, Y.reshape(-1, 1), sample_weight.reshape(-1, 1)), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "    \n",
    "    def predict(self, X):  \n",
    "        print(\"X shape\",X.shape)\n",
    "        predictions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return predictions\n",
    "    \n",
    "    def make_prediction(self, x, tree):\n",
    "        if tree.value != None:\n",
    "            return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val <= tree.threshold:\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)\n",
    "\n",
    "\n",
    "# Initialize arrays to store alphas and predictions\n",
    "alphas = []\n",
    "all_predictions = []\n",
    "accuracies = []  # To store accuracies on the validation set\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.ones(len(y_train[:3000]))  # Initialize weights uniformly\n",
    "\n",
    "# Iterate to grow 300 decision stumps\n",
    "for i in range(300):\n",
    "    # Build decision stump\n",
    "    decision_stump = DecisionTreeClassifier(min_samples_split=2, max_depth=1)\n",
    "    decision_stump.fit(X_reduced[:3000], y_train[:3000], sample_weight=weights)  # Use updated weights\n",
    "    predictions = decision_stump.predict(X_reduced[:3000])\n",
    "\n",
    "    # Calculate error rate\n",
    "    error_rate = np.sum(weights * (predictions != y_train[:3000])) / np.sum(weights)\n",
    "\n",
    "    # Calculate alpha\n",
    "    alpha = 0.5 * np.log((1 - error_rate) / error_rate)\n",
    "    alphas.append(alpha)\n",
    "\n",
    "    # Update weights\n",
    "    for j in range(len(weights)):\n",
    "        if predictions[j] == y_train[j]:\n",
    "            weights[j] *= np.exp(-alpha)\n",
    "        else:\n",
    "            weights[j] *= np.exp(alpha)\n",
    "\n",
    "    # Normalize weights\n",
    "    weights /= np.sum(weights)\n",
    "    \n",
    "    # Store predictions for later evaluation\n",
    "    all_predictions.append(predictions)\n",
    "    \n",
    "    # Compute accuracy on the validation set\n",
    "    val_predictions = decision_stump.predict(x_reduced_val)\n",
    "    val_accuracy = np.mean(val_predictions == y_val)\n",
    "    accuracies.append(val_accuracy)\n",
    "    \n",
    "    # Print alpha and h(2) for each iteration\n",
    "    print(f\"Iteration {i+1}: Alpha = {alpha}, h({i+1}) = {decision_stump}, Validation Accuracy = {val_accuracy}\")\n",
    "\n",
    "# Plot accuracy vs. number of trees\n",
    "plt.plot(range(1, 301), accuracies)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Accuracy on Validation Set')\n",
    "plt.title('Accuracy vs. Number of Trees')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
